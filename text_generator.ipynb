{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE 1/6\n",
    "### Objectives:\n",
    "In order to prepare the corpus for use in this project, we need to take the following important steps:\n",
    "\n",
    "* Open and read the corpus from the provided file corpus.txt. The filename should be specified as user input. Note that the file is written in UTF-8 encoding, and the file should be in the same folder as your Python script.\n",
    "* Break the corpus into individual words. To create a Markov model, we use the simplest form of tokenization: tokens are separated by whitespace characters such as spaces, tabulation, and newline characters. Punctuation marks should be left untouched since later on, they will play an important role in signaling where a sentence should end.\n",
    "* Acquire and print the following information about the corpus under the section of the output called Corpus statistics:\n",
    "\n",
    "— the number of all tokens;\n",
    "\n",
    "— the number of all unique tokens, that is, the number of tokens without repetition.\n",
    "\n",
    "Each of the above should be in a new line.\n",
    "* Take an integer as user input and print the token with the corresponding index. Repeat this process until the string exit is input. Also, make sure that the input index is actually an integer that falls in the range of the corpus. If that is not the case, print an error message and request a new input. Error messages should contain the types of errors (Type Error, Index Error, etc.).\n",
    "\n",
    "\n",
    "Each token should be printed in a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.txt\n",
      "Corpus statistics\n",
      "All tokens: 287968\n",
      "Unique tokens: 21262\n",
      "56789\n",
      "I\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "# from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "with open(input(), 'r', encoding='utf-8') as f:  # input() = 'corpus.txt'\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "# corpus = regexp_tokenize(text, \"[^\\s]+\")\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "corpus = tk.tokenize(text)\n",
    "print(f'Corpus statistics\\nAll tokens: {len(corpus)}\\nUnique tokens: {len(set(corpus))}')\n",
    "\n",
    "i = input()  # Type number\n",
    "while i != 'exit':\n",
    "    try:\n",
    "        print(corpus[int(i)])\n",
    "    except IndexError:\n",
    "        print('IndexError. Please input an integer that is in the range of the corpus.')\n",
    "    except ValueError:\n",
    "        print('TypeError. Please input an integer.')\n",
    "    i = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE 2/6\n",
    "\n",
    "### Objectives:\n",
    "* Transform the corpus into a collection of bigrams. The results should contain all the possible bigrams from the corpus, which means that:\n",
    "— Every token from the corpus should be a head of a bigram with the exception of the last token which cannot become a head since nothing follows it;\n",
    "— Every token from the corpus should be a tail of a bigram with the exception of the first token which cannot possibly be the tail of a bigram because nothing precedes it.\n",
    "* Output the number of all bigrams in the corpus.\n",
    "* Take an integer as user input and print the bigrams with the corresponding index. Repeat this process until the string exit is input. Also, make sure that the input is actually an integer that falls in the range of the collection of bigrams. If that is not the case, print an error message and request a new input. Error messages should contain the types of errors (Type Error, Index Error, etc.). Each bigram should have the format Head: [head] Tail: [tail] and should be printed in a new line.\n",
    "\n",
    "\n",
    "You should only print the output of the current stage and not the previous one, but like in the previous stage, the name of the file that contains the corpus should be given as user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.txt\n",
      "Number of bigrams:: 287967\n",
      "45\n",
      "Head: life. Tail: How\n",
      "46\n",
      "Head: How Tail: close\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "# from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "with open(input(), 'r', encoding='utf-8') as f:  # заменять input() на 'corpus.txt'\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "# corpus = regexp_tokenize(text, \"[^\\s]+\")\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "corpus = tk.tokenize(text)\n",
    "# print(f'Corpus statistics\\nAll tokens: {len(corpus)}\\nUnique tokens: {len(set(corpus))}')\n",
    "\n",
    "bigrams = [[corpus[i], corpus[i + 1]] for i in range(len(corpus) - 1)]\n",
    "print(f'Number of bigrams:: {len(bigrams)}')\n",
    "\n",
    "\n",
    "i = input()  # Type number\n",
    "while i != 'exit':\n",
    "    try:\n",
    "        print(f'Head: {bigrams[int(i)][0]} Tail: {bigrams[int(i)][1]}')\n",
    "    except IndexError:\n",
    "        print('IndexError. Please input an integer that is in the range of the corpus.')\n",
    "    except ValueError:\n",
    "        print('TypeError. Please input an integer.')\n",
    "    i = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE 3/6\n",
    "\n",
    "### Description:\n",
    "This is the final step where we will work on creating a Markov chain model. We will use the data prepared in the first two stages and transform it into a model. This model will contain probabilistic information that will tell us what the next word in a chain might be.\n",
    "\n",
    "We already have a list of all bigrams from the corpus. As we discussed earlier, this can already be used to make some naive predictions. There is a problem, though: right now our data contains a lot of repetition. As we have seen at the first stage, the total number of tokens is almost 10 times greater than the number of unique tokens. This ratio must be about the same in the list of bigrams. Some bigrams might be very common, others — relatively rare. At the moment, we have no way of telling which are which.\n",
    "\n",
    "To resolve this problem, we will make a simplified version of a Markov chain model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.txt\n",
      "queen\n",
      "Head: queen\n",
      "Tail: and Count: 14\n",
      "Tail: has Count: 5\n",
      "Tail: someday. Count: 1\n",
      "Tail: today. Count: 1\n",
      "Tail: of Count: 8\n",
      "Tail: one Count: 1\n",
      "Tail: over Count: 1\n",
      "Tail: you Count: 2\n",
      "Tail: doesn't Count: 2\n",
      "Tail: waiting. Count: 1\n",
      "Tail: herself Count: 1\n",
      "Tail: regent. Count: 1\n",
      "Tail: I'm Count: 2\n",
      "Tail: when Count: 2\n",
      "Tail: or Count: 2\n",
      "Tail: at Count: 1\n",
      "Tail: said Count: 1\n",
      "Tail: how Count: 1\n",
      "Tail: detained Count: 1\n",
      "Tail: get Count: 1\n",
      "Tail: a Count: 2\n",
      "Tail: to Count: 9\n",
      "Tail: would Count: 2\n",
      "Tail: is Count: 5\n",
      "Tail: whose Count: 1\n",
      "Tail: with Count: 1\n",
      "Tail: mother Count: 2\n",
      "Tail: who Count: 3\n",
      "Tail: ordered Count: 1\n",
      "Tail: trusts Count: 1\n",
      "Tail: I Count: 1\n",
      "Tail: Can't Count: 1\n",
      "Tail: You Count: 1\n",
      "Tail: because Count: 3\n",
      "Tail: tried Count: 1\n",
      "Tail: recognizes Count: 1\n",
      "Tail: chose Count: 2\n",
      "Tail: She's Count: 2\n",
      "Tail: Margaery's Count: 1\n",
      "Tail: will Count: 3\n",
      "Tail: loves Count: 1\n",
      "Tail: returns. Count: 1\n",
      "Tail: does Count: 2\n",
      "Tail: insists Count: 1\n",
      "Tail: before? Count: 1\n",
      "Tail: mother's Count: 1\n",
      "Tail: mother? Count: 1\n",
      "Tail: in Count: 2\n",
      "Tail: summons Count: 1\n",
      "Tail: now. Count: 2\n",
      "Tail: demands. Count: 1\n",
      "Tail: knows Count: 1\n",
      "Tail: thought Count: 1\n",
      "Tail: am Count: 1\n",
      "Tail: we Count: 1\n",
      "Tail: until Count: 1\n",
      "Tail: can Count: 1\n",
      "Tail: needs Count: 1\n",
      "Tail: Doesn't Count: 1\n",
      "Tail: did Count: 1\n",
      "Tail: respects Count: 1\n",
      "Tail: warm. Count: 1\n",
      "Tail: too. Count: 1\n",
      "Tail: herself? Count: 1\n",
      "Tail: out Count: 1\n",
      "Tail: got Count: 1\n",
      "Tail: slaughtered Count: 1\n",
      "Tail: wanted Count: 1\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "with open(input(), 'r', encoding='utf-8') as f:  # заменять input() на 'corpus.txt'\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "corpus = tk.tokenize(text)\n",
    "\n",
    "bigrams = [[corpus[i], corpus[i + 1]] for i in range(len(corpus) - 1)]\n",
    "\n",
    "heads_dict = {}\n",
    "\n",
    "for head, tail in bigrams:\n",
    "    heads_dict.setdefault(head, []).append(tail)\n",
    "\n",
    "word = input()  # Type your word\n",
    "while word != 'exit':\n",
    "    try:\n",
    "        freq_counter = Counter(heads_dict[word])\n",
    "        print(f'Head: {word}', *[f'Tail: {key} Count: {value}' for key, value in freq_counter.items()], sep='\\n')\n",
    "    except KeyError:\n",
    "        print(f'Head: {word}', 'The requested word is not in the model. Please input another word.', sep='\\n')\n",
    "    finally:\n",
    "        word = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE 4/6\n",
    "\n",
    "### Objectives:\n",
    "* Choose a random word from the corpus that will serve as the first word of the chain.\n",
    "* The second word should be predicted by looking up the first word of the chain in the model and choosing the most probable next word from the set of possible follow-ups. Right now, an entry contains all the possible tails that might follow the selected head along with their corresponding repetition counts. Using the repetition counts, you will be able to choose the most probable option.\n",
    "* The second step should be repeated until the length of the chain is 10 words, but this time, the current last word of the chain should be used to look up another probable word to continue the sentence.\n",
    "Using the algorithm described above, generate chains consisting of 10 tokens, join the resulting tokens together, and print them as a pseudo-sentence. Keep in mind that a pseudo-sentence can consist of multiple actual sentences, so having punctuation marks inside pseudo-sentences is completely valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.txt\n",
      "certain about that prerogative. I will pardon these fanatics of\n",
      "Come here. Are you into the brink of shoes. But\n",
      "on you, but Children do without. She came to celebrate.\n",
      "the edge of Light demands this spread if you going\n",
      "my friend... Voices carry weapons and band of his sword\n",
      "only here a cook. But the free rides off me!\n",
      "Welcome. Stop. I told help to be here at the\n",
      "we? Just don't want me to your sister and I\n",
      "hope I wonder, Ser Meryn. Go to him anyway? You\n",
      "and warm. The Iron Bank wants to King's Landing, I'll\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "with open(input(), 'r', encoding='utf-8') as f:  # заменять input() на 'corpus.txt'\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "corpus = tk.tokenize(text)\n",
    "\n",
    "bigrams = [[corpus[i], corpus[i + 1]] for i in range(len(corpus) - 1)]\n",
    "\n",
    "\n",
    "heads_dict = {}\n",
    "\n",
    "for head, tail in bigrams:\n",
    "    heads_dict.setdefault(head, []).append(tail)\n",
    "\n",
    "    \n",
    "text = []\n",
    "for i in range(10):\n",
    "    \n",
    "    sentence = []\n",
    "    beginning = random.choice(corpus)\n",
    "    sentence.append(beginning)\n",
    "    \n",
    "    for j in range(9):\n",
    "        freq_counter = Counter(heads_dict[beginning])\n",
    "        \n",
    "        population = []\n",
    "        weights = []\n",
    "\n",
    "        for key, value in freq_counter.items():\n",
    "            population.append(key)\n",
    "            weights.append(value)\n",
    "            \n",
    "        beginning = random.choices(population, weights)[0]\n",
    "        sentence.append(beginning)\n",
    "        \n",
    "#     print(sentence)\n",
    "    \n",
    "    text.append(sentence)\n",
    "\n",
    "for el in text:\n",
    "    print(*el)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE 5/6\n",
    "\n",
    "### Objectives:\n",
    "* Make the algorithm more realistic by generating pseudo-sentences instead of just random text.\n",
    "The sentences that are being generated should:\n",
    "— always start with capitalized words (\"This is beautiful.\", \"You are a great programmer!\", etc.);\n",
    "— not start with a word that ends with a sentence-ending punctuation mark (\"Okay.\", \"Nice.\", \"Good.\", \"Look!\", \"Jon!\", etc.);\n",
    "— always end with a sentence-ending punctuation mark like ., !, or ?;\n",
    "— should not be shorter than 5 tokens.\n",
    "* Generate and print exactly 10 pseudo-sentences that meet these criteria. A pseudo-sentence should end when the first sentence-ending punctuation mark is encountered after the minimal sentence length (5 tokens) is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't want to Oldtown, then we'll have sneered at you.\n",
      "I'm Jojen told me stand behind small matters to kill.\n",
      "I knew about being rude.\n",
      "I come to her? I admired her.\n",
      "Kingdoms, I said-- Turn around my friend.\n",
      "There's nothing you were said he took them.\n",
      "So how to admit fear.\n",
      "The dark-haired one. Next I want Balon Greyjoy because she's doing.\n",
      "What's happening? The maesters whose skeleton sits on you.\n",
      "I love once. No, my lady.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from string import ascii_uppercase\n",
    "\n",
    "def read_file(file_):\n",
    "    with open(file_, 'r', encoding='utf-8') as f:  # заменять input() на 'corpus.txt'\n",
    "        raw_text = f.read()\n",
    "        f.close()\n",
    "    return raw_text\n",
    "\n",
    "name_file = 'corpus.txt'\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "corpus = tk.tokenize(read_file(name_file))\n",
    "\n",
    "bigrams = [[corpus[i], corpus[i + 1]] for i in range(len(corpus) - 1)]\n",
    "\n",
    "heads_dict = {}\n",
    "\n",
    "for head, tail in bigrams:\n",
    "    heads_dict.setdefault(head, []).append(tail)\n",
    "\n",
    "def pop_wei(heads_dict, beginning):\n",
    "    freq_counter = Counter(heads_dict[beginning])\n",
    "    population = []\n",
    "    weights = []\n",
    "\n",
    "    for key, value in freq_counter.items():\n",
    "        population.append(key)\n",
    "        weights.append(value)\n",
    "\n",
    "    return [population, weights]\n",
    "\n",
    "def print_sentences(text):\n",
    "    for el in text:\n",
    "        print(*el)\n",
    "\n",
    "def upper_cased(corpus):\n",
    "    return list(filter(lambda x: x and x[0] in ascii_uppercase and not re.findall(r'[.!?]', x[-1]), corpus))\n",
    "\n",
    "\n",
    "text = []\n",
    "\n",
    "for i in range(10):\n",
    "    sentence = []\n",
    "    beginning = random.choice(upper_cased(corpus))\n",
    "    sentence.append(beginning)\n",
    "\n",
    "    while not re.findall(r'[.!?]', beginning) or len(sentence) < 5:\n",
    "        beginning = random.choices(pop_wei(heads_dict, beginning)[0], pop_wei(heads_dict, beginning)[1])[0]\n",
    "        sentence.append(beginning)\n",
    "\n",
    "    text.append(sentence)\n",
    "\n",
    "print_sentences(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE 6/6\n",
    "\n",
    "Right now, the model is based on bigrams, that is, we only consider one word when trying to predict the next word in the chain.\n",
    "The algorithm should be extended so that it can use not only bigrams but also trigrams. \n",
    "### This change implies the following tasks:\n",
    "* The list of bigrams should be transformed into a list of trigrams. It should still consist of heads and tails, but now, heads should consist of two space-separated tokens concatenated into a single string. The tails should still consist of one token. For example: head — winter is, tail — coming.\n",
    "\n",
    "* The model should be trained based on the list of trigrams. The model creation requires no modifications since trigrams still consist of a head and a tail.\n",
    "\n",
    "* The beginning of the chain should be a randomly chosen head from the model, not just any word from the corpus.\n",
    "\n",
    "* When predicting the next word, the model should be fed the concatenation of the last two tokens of the chain separated by a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A legendary fighter. A brilliant stylist who invented half the country will rally to their own.\n",
      "They would have been searching for you.\n",
      "Fine leather, ornamentation, detailing, and time Time most of them did this to her.\n",
      "You, me-- we. Once I kill you with your wildling lovers.\n",
      "Soon they won't just rule over the city who doesn't like the silly little boy when I watched the witch burn.\n",
      "Kept me in the Night's Watch.\n",
      "The Halfhand believed our only chance It's a compliment, my lady.\n",
      "Save your lies for court.\n",
      "He'd kill us all. It's not my people.\n",
      "Please, someone stop him! My lord, this man murdered him.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from string import ascii_uppercase\n",
    "\n",
    "def read_file(name_file):\n",
    "    with open(name_file, 'r', encoding='utf-8') as f:  # заменять input() на 'corpus.txt'\n",
    "        raw_text = f.read()\n",
    "        f.close()\n",
    "    return raw_text\n",
    "\n",
    "# name_file = input()\n",
    "name_file = 'corpus.txt'\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "corpus = tk.tokenize(read_file(name_file))\n",
    "\n",
    "trigrams = [[corpus[i] + ' ' + corpus[i + 1], corpus[i + 2]] for i in range(len(corpus) - 2)]\n",
    "\n",
    "heads_dict = {}\n",
    "\n",
    "for head, tail in trigrams:\n",
    "    heads_dict.setdefault(head, []).append(tail)\n",
    "\n",
    "def pop_wei(heads_dict, beginning):\n",
    "    freq_counter = Counter(heads_dict[beginning])\n",
    "    population = []\n",
    "    weights = []\n",
    "\n",
    "    for key, value in freq_counter.items():\n",
    "        population.append(key)\n",
    "        weights.append(value)\n",
    "\n",
    "    return [population, weights]\n",
    "\n",
    "def print_sentences(text):\n",
    "    for el in text:\n",
    "        print(*el)\n",
    "\n",
    "def upper_cased(corpus):\n",
    "    return list(filter(lambda x: x and x[0] in ascii_uppercase and not re.findall(r'[.!?]', x), corpus))\n",
    "\n",
    "\n",
    "text = []\n",
    "\n",
    "for i in range(10):\n",
    "    sentence = []\n",
    "    beginning = random.choice(upper_cased(list(heads_dict.keys())))\n",
    "    sentence.append(beginning)\n",
    "\n",
    "    while len(sentence) < 5 or not re.findall(r'[.!?] ', beginning):\n",
    "        beginning = random.choices(pop_wei(heads_dict, beginning)[0], pop_wei(heads_dict, beginning)[1])[0]\n",
    "        sentence.append(beginning)\n",
    "        beginning = sentence[-2].split(' ')[-1] + ' ' + sentence[-1]\n",
    "\n",
    "    text.append(sentence[:-1])\n",
    "\n",
    "print_sentences(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
